services:
  timescaledb:
    image: timescale/timescaledb-ha:pg17
    container_name: timescaledb
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
      # Scripts in this folder run once on first container start
      - ./docker/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ── Ollama: local LLM inference server (optional) ──────────
  # Serves Qwen3, BGE-M3, and other models via OpenAI-compatible API.
  # To use: set AI_PROVIDER=openai_compatible and AI_BASE_URL=http://ollama:11434/v1
  # Models are downloaded on first use: docker compose exec ollama ollama pull qwen3:8b
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # Uncomment for GPU support (NVIDIA):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    restart: unless-stopped
    profiles:
      - ollama   # Only starts with: docker compose --profile ollama up -d

volumes:
  timescaledb_data:
  ollama_data: